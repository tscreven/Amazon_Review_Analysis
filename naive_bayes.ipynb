{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     /Users/tscreven/nltk_data...\n",
      "[nltk_data]   Package tagsets_json is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tscreven/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from methods import *\n",
    "from collections import Counter\n",
    "import math\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_NAMES = basenames()\n",
    "VOTE_THRESHOLD = 1\n",
    "SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(filename):\n",
    "    text, labels = get_data(filename, VOTE_THRESHOLD)\n",
    "    train_text, train_labels, test_text, test_labels = split_data(text, labels, SPLIT)\n",
    "\n",
    "    prob_helpful = sum(train_labels) / len(train_labels) # positive class labeled as 1\n",
    "    prob_unhelpful = 1 - prob_helpful\n",
    "    helpful_fd = nltk.FreqDist()\n",
    "    unhelpful_fd = nltk.FreqDist()\n",
    "    for review_text, helpful in zip(train_text, train_labels):\n",
    "        tokens = word_tokenize(review_text)\n",
    "        if helpful: helpful_fd.update(tokens)\n",
    "        else: unhelpful_fd.update(tokens)\n",
    "    return helpful_fd, unhelpful_fd, prob_helpful, prob_unhelpful, test_text, test_labels, len(train_text), len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_log_probability(prior_prob, fd, class_num_words, review_tokens, total_vocab_size):\n",
    "    terms = [math.log(prior_prob)]\n",
    "    for token in review_tokens:\n",
    "        prob = math.log((fd[token] + 1) / (class_num_words + total_vocab_size))\n",
    "        terms.append(prob)\n",
    "    return sum(terms) # Python sum() over a list is more accurate than incremently adding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_log_probabilities(helpful_log_prob, unhelpful_log_prob):\n",
    "    helpful_log_prob -= unhelpful_log_prob\n",
    "    try:\n",
    "        helpful_prob = math.exp(helpful_log_prob)\n",
    "    except:\n",
    "        return 1.0\n",
    "    return helpful_prob / (helpful_prob + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(filename):\n",
    "    helpful_fd, unhelpful_fd, prob_helpful, prob_unhelpful, test_text, test_labels, train_length, test_length = get_parameters(filename)\n",
    "    vocabulary = set(helpful_fd.keys()).union(set(unhelpful_fd.keys()))\n",
    "    total_vocab_size = len(vocabulary)\n",
    "    num_correct = 0\n",
    "    results = []\n",
    "    helpful_class_size = sum(helpful_fd.values())\n",
    "    unhelpful_class_size = sum(unhelpful_fd.values())\n",
    "    for rev_text, label in zip(test_text, test_labels):\n",
    "        tokens = word_tokenize(rev_text)\n",
    "        unnorm_helpful_prob = class_log_probability(prob_helpful, helpful_fd, helpful_class_size, tokens, total_vocab_size)\n",
    "        unnorm_unhelpful_prob = class_log_probability(prob_unhelpful, unhelpful_fd, unhelpful_class_size, tokens, total_vocab_size)\n",
    "        helpful_prob = normalize_log_probabilities(unnorm_helpful_prob, unnorm_unhelpful_prob)\n",
    "        prediction = unnorm_helpful_prob >= unnorm_unhelpful_prob\n",
    "        results.append((helpful_prob, prediction, label))\n",
    "        if unnorm_helpful_prob >= unnorm_unhelpful_prob and label == 1:\n",
    "            num_correct += 1\n",
    "        elif unnorm_unhelpful_prob > unnorm_helpful_prob and label == 0:\n",
    "            num_correct += 1\n",
    "\n",
    "    results = sorted(results, key=lambda x: x[0], reverse=True)\n",
    "    top_size = int(len(results) * 0.05)\n",
    "    top_percent = results[:top_size]\n",
    "    true_positives = sum(1 for _, _, label in top_percent if label == 1)\n",
    "    precision_top_5_percent = true_positives / top_size\n",
    "    return precision_top_5_percent, prob_helpful, train_length, test_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n",
      "0.6904761904761905\n",
      "0.6187563710499491\n",
      "0.4834503510531595\n",
      "0.6586586586586587\n",
      "0.8176352705410822\n",
      "0.4164164164164164\n",
      "0.3863863863863864\n",
      "0.3569868995633188\n",
      "0.6176176176176176\n",
      "0.20689655172413793\n",
      "0.5256281407035176\n",
      "0.5205205205205206\n",
      "0.6090534979423868\n",
      "0.35035035035035034\n",
      "0.5181518151815182\n",
      "0.8260869565217391\n",
      "0.6766766766766766\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "prior_helpful_probs = []\n",
    "train_sizes = []\n",
    "test_sizes = []\n",
    "\n",
    "for base in BASE_NAMES:\n",
    "    prec, prob_helpful, train_len, test_len = run(get_decimated_name(base))\n",
    "    print(prec)\n",
    "    precisions.append(prec)\n",
    "    prior_helpful_probs.append(prob_helpful)\n",
    "    train_sizes.append(train_len)\n",
    "    test_sizes.append(test_len)\n",
    "\n",
    "delta = [precisions[i] - prior_helpful_probs[i] for i in range(len(BASE_NAMES))]\n",
    "ratio = [precisions[i] / prior_helpful_probs[i] for i in range(len(BASE_NAMES))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
